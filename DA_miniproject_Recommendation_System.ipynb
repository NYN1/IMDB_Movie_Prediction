{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "DA_miniproject_Recommendation_System.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG_nsvt0EqWB"
      },
      "source": [
        "#CONTENT BASED FILTERING FOR ENGLISH LANGUAGE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCYrRk1MEjYq",
        "outputId": "a41d5932-42fb-4d4b-ccd7-8b7e19f9ccfb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fexperimentsandconfigs%20https%3a%2f%2fwww.googleapis.com%2fauth%2fphotos.native&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/1AY0e-g4ngOxnGx9U2ixi_AOLRWtLhDEqsS6-8pW6jauiv7dVsqBQL8q4ZU4\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmjtgxBeE2OF",
        "outputId": "cdac183f-d19b-420f-9ab4-5269d8064708"
      },
      "source": [
        "cd 'drive/MyDrive/Kaggle'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'drive/MyDrive/Kaggle'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4JBROztkGt8",
        "outputId": "5b3e2ddd-82ab-4f64-b0e7-d07f174aec8f"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "description.csv  kaggle.json  movies.csv  ratings.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "y_-GcngOl06I",
        "outputId": "8a19f918-ca77-4e98-cd0f-79c9693cf90a"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords  \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.stem import PorterStemmer\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS \n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "\n",
        "movies = pd.read_csv(\"movies.csv\")\n",
        "ratings= pd.read_csv(\"ratings.csv\")\n",
        "description= pd.read_csv(\"description.csv\")\n",
        "\n",
        "\n",
        "movies_j = movies[[\"imdb_title_id\",\"original_title\",\"year\",\"language\",\"genre\",\"duration\",\"country\",\"production_company\",\"director\",\"writer\",\"actors\",\"description\",\"reviews_from_users\",\"reviews_from_critics\"]]\n",
        "names_j = names[[\"imdb_name_id\",\"name\"]]\n",
        "ratings_j = ratings[[\"imdb_title_id\",\"weighted_average_vote\",\"mean_vote\",\"median_vote\",\"top1000_voters_rating\"]]\n",
        "link_j = titles[[\"imdb_title_id\",\"imdb_name_id\",\"category\",\"characters\"]]\n",
        "imdb = pd.merge(movies_j, ratings_j, how='inner', on=\"imdb_title_id\",sort=True)\n",
        "imdb = imdb.drop([\"duration\",\"country\"],axis=1)\n",
        "\n",
        "imdb_eng = imdb[imdb[\"language\"] == \"English\" ]\n",
        "imdb_eng.tail()\n",
        "imdb_eng.dropna(inplace= True)\n",
        "print(imdb_eng.isnull().values.any())\n",
        "print(len(imdb_eng))\n",
        "imdb_2 = imdb_eng.iloc[15000:]\n",
        "\n",
        "test = imdb_2.reset_index(drop = True)\n",
        "test = test.drop(\"imdb_title_id\",axis = 1)\n",
        "test.set_index(\"original_title\",drop = True,inplace = True)\n",
        "test[\"genre\"] = test[\"genre\"].str.split(\",\")\n",
        "test[\"writer\"] = test[\"writer\"].str.split(',')\n",
        "test[\"actors\"] = test[\"actors\"].str.split(',')\n",
        "test[\"director\"] = test[\"director\"].str.split(',')\n",
        "test[\"production_company\"] = test[\"production_company\"].str.split(',')\n",
        "\n",
        "stop_w = set(stopwords.words('english'))\n",
        "stop_words = []\n",
        "for word in stop_w:\n",
        "  stop_words.append(word)\n",
        "wordcloud = WordCloud(width = 800, height = 800, \n",
        "                background_color ='white', \n",
        "                stopwords = stop_words, \n",
        "                min_font_size = 10).generate(comment_words)\n",
        "plt.figure(figsize = (8, 8), facecolor = None) \n",
        "plt.imshow(wordcloud) \n",
        "plt.axis(\"off\") \n",
        "plt.tight_layout(pad = 0) \n",
        "plt.show() \n",
        "lemmatizer = WordNetLemmatizer() \n",
        "ps = PorterStemmer() \n",
        "i=0\n",
        "stop_copy=set(stop_words)\n",
        "\n",
        "i = 0\n",
        "for text in test[\"description\"]:\n",
        "  text=str(text)\n",
        "  text=set(text.split(\" \"))\n",
        "  text=text-stop_copy\n",
        "  a = list(text)\n",
        "  w = []\n",
        "  for word in a:\n",
        "    word_s = ps.stem(word)\n",
        "    word_l = lemmatizer.lemmatize(word_s)\n",
        "    w.append(word_l)\n",
        "  sentence = \"\"\n",
        "  for word in w:\n",
        "    sentence = sentence + word\n",
        "    sentence = sentence + \" \"\n",
        "  test[\"description\"][i]= sentence\n",
        "  i=i+1\n",
        "\n",
        "#test.head()\n",
        "#production_company\n",
        "#director\n",
        "#writer\n",
        "#actors\n",
        "#reviews_from_users\n",
        "#reviews_from_critics\n",
        "\n",
        "test.dropna(inplace= True)\n",
        "test.isnull().values.any()\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfv = TfidfVectorizer(min_df = 3 , max_features = None , strip_accents = \"unicode\", analyzer = \"word\" , ngram_range = (1,3))\n",
        "test2 = test\n",
        "test2[\"description\"] = test2[\"description\"].fillna(\" \")\n",
        "tfv_matrix = tfv.fit_transform(test2[\"description\"])\n",
        "tfv_matrix.shape\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "sigmoid_sim = cosine_similarity(tfv_matrix, tfv_matrix)\n",
        "\n",
        "dic1 = {}\n",
        "dic2 = {}\n",
        "indices = pd.DataFrame(test.index)\n",
        "for i,text in enumerate(indices[\"original_title\"]):\n",
        "  dic1[text] = i\n",
        "  dic2[i] = text\n",
        "print(dic1[\"True Blue\"])\n",
        "print(dic2[1])\n",
        "indices.head()\n",
        "#sigmoid_sim[0]\n",
        "\n",
        "def recommend_system(title,sigmoid_sim):\n",
        "  index = dic1[title]\n",
        "  sig_scores = list(enumerate(sigmoid_sim[index]))\n",
        "  scores = sorted(sig_scores,key= lambda x : x[1],reverse=True)\n",
        "  scores = scores[1:11]\n",
        "  movies_indics = [i[0] for i in scores]\n",
        "  movies = [dic2[i] for i in movies_indics]\n",
        "  return movies\n",
        "\n",
        "a = recommend_system(\"The Samaritan\",sigmoid_sim)\n",
        "print(a)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-6f2a8575e16f>\"\u001b[0;36m, line \u001b[0;32m121\u001b[0m\n\u001b[0;31m    print(a)'''\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ugYmzM9E8Td"
      },
      "source": [
        "#CONTENT BASED FILTERING FOR INDIAN MOVIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhWu5tNeCNN7"
      },
      "source": [
        "'''import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords  \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "movies = pd.read_csv(\"/content/IMDb movies.csv\")\n",
        "names = pd.read_csv(\"/content/IMDb names.csv\")\n",
        "ratings = pd.read_csv(\"/content/IMDb ratings.csv\")\n",
        "titles = pd.read_csv(\"/content/IMDb title_principals.csv\")\n",
        "\n",
        "movies_j = movies[[\"imdb_title_id\",\"original_title\",\"year\",\"language\",\"genre\",\"duration\",\"country\",\"production_company\",\"director\",\"writer\",\"actors\",\"description\",\"reviews_from_users\",\"reviews_from_critics\"]]\n",
        "names_j = names[[\"imdb_name_id\",\"name\"]]\n",
        "ratings_j = ratings[[\"imdb_title_id\",\"weighted_average_vote\",\"mean_vote\",\"median_vote\",\"top1000_voters_rating\"]]\n",
        "link_j = titles[[\"imdb_title_id\",\"imdb_name_id\",\"category\",\"characters\"]]\n",
        "imdb = pd.merge(movies_j, ratings_j, how='inner', on=\"imdb_title_id\",sort=True)\n",
        "imdb = imdb.drop([\"duration\"],axis=1)\n",
        "\n",
        "imdb_indian = imdb[imdb[\"country\"] == \"India\"]\n",
        "print(imdb_indian.isnull().values.any())\n",
        "imdb_indian.dropna(inplace= True)\n",
        "print(imdb_indian.isnull().values.any())\n",
        "print(len(imdb_indian))\n",
        "print(\"\\n\")\n",
        "\n",
        "#imdb_indian.tail(100)\n",
        "imdb_telugu = imdb_indian[imdb_indian[\"language\"] == \"Telugu\"]\n",
        "#imdb_telugu.tail(100)\n",
        "\n",
        "test = imdb_indian.reset_index(drop = True)\n",
        "test = test.drop(\"imdb_title_id\",axis = 1)\n",
        "test.set_index(\"original_title\",drop = True,inplace = True)\n",
        "test[\"genre\"] = test[\"genre\"].str.split(\",\")\n",
        "test[\"writer\"] = test[\"writer\"].str.split(',')\n",
        "test[\"actors\"] = test[\"actors\"].str.split(',')\n",
        "test[\"director\"] = test[\"director\"].str.split(',')\n",
        "test[\"production_company\"] = test[\"production_company\"].str.split(',')\n",
        "\n",
        "stop_w = set(stopwords.words('english'))\n",
        "stop_words = []\n",
        "for word in stop_w:\n",
        "  stop_words.append(word)\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "ps = PorterStemmer() \n",
        "i=0\n",
        "stop_copy=set(stop_words)\n",
        "\n",
        "i = 0\n",
        "for text in test[\"description\"]:\n",
        "  text=str(text)\n",
        "  text=set(text.split(\" \"))\n",
        "  text=text-stop_copy\n",
        "  a = list(text)\n",
        "  w = []\n",
        "  for word in a:\n",
        "    word_s = ps.stem(word)\n",
        "    word_l = lemmatizer.lemmatize(word_s)\n",
        "    w.append(word_l)\n",
        "  sentence = \"\"\n",
        "  for word in w:\n",
        "    sentence = sentence + word\n",
        "    sentence = sentence + \" \"\n",
        "  test[\"description\"][i]= sentence\n",
        "  i=i+1\n",
        "\n",
        "test.dropna(inplace= True)\n",
        "test.isnull().values.any()\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfv = TfidfVectorizer(min_df = 3 , max_features = None , strip_accents = \"unicode\", analyzer = \"word\" , ngram_range = (1,3))\n",
        "test2 = test\n",
        "test2[\"description\"] = test2[\"description\"].fillna(\" \")\n",
        "tfv_matrix = tfv.fit_transform(test2[\"description\"])\n",
        "tfv_matrix.shape\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "sigmoid_sim = cosine_similarity(tfv_matrix, tfv_matrix)\n",
        "\n",
        "dic1 = {}\n",
        "dic2 = {}\n",
        "indices = pd.DataFrame(test.index)\n",
        "for i,text in enumerate(indices[\"original_title\"]):\n",
        "  dic1[text] = i\n",
        "  dic2[i] = text\n",
        "print(dic1[\"Andaz\"])\n",
        "print(dic2[1])\n",
        "indices.head()\n",
        "#sigmoid_sim[0]\n",
        "\n",
        "def recommend_system(title,sigmoid_sim):\n",
        "  index = dic1[title]\n",
        "  sig_scores = list(enumerate(sigmoid_sim[index]))\n",
        "  scores = sorted(sig_scores,key= lambda x : x[1],reverse=True)\n",
        "  scores = scores[1:11]\n",
        "  movies_indics = [i[0] for i in scores]\n",
        "  movies = [dic2[i] for i in movies_indics]\n",
        "  return movies\n",
        "\n",
        "a = recommend_system(\"Gang Leader\",sigmoid_sim)\n",
        "print(a)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHrgxktlHXtb"
      },
      "source": [
        "'''def recommend_system(title,sigmoid_sim):\n",
        "  index = dic1[title]\n",
        "  sig_scores = list(enumerate(sigmoid_sim[index]))\n",
        "  scores = sorted(sig_scores,key= lambda x : x[1],reverse=True)\n",
        "  scores = scores[1:11]\n",
        "  movies_indics = [i[0] for i in scores]\n",
        "  movies = [dic2[i] for i in movies_indics]\n",
        "  return movies\n",
        "\n",
        "a = recommend_system(\"Gang Leader\",sigmoid_sim)\n",
        "print(a)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpubhYe9HrN4"
      },
      "source": [
        "imdb_telugu.tail(1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXqKfVeqJGPl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLoldLPnfmj5"
      },
      "source": [
        "def sort_movies_by_year(li):\n",
        "    def merge(a,l,mid,r):\n",
        "        n1=mid-l+1\n",
        "        n2=r-(mid+1)+1\n",
        "        L=[a[i+l] for i in range(n1)]\n",
        "        R=[a[i+mid+1] for i in range(n2)]\n",
        "        i,j,k=0,0,l\n",
        "        while(i<n1 and j<n2):\n",
        "            if int(L[i][-5:-1])>int(R[j][-5:-1]) :\n",
        "                a[k]=L[i]\n",
        "                i+=1\n",
        "            else:\n",
        "                a[k]=R[j]\n",
        "                j+=1\n",
        "            k+=1\n",
        "        while(i<n1):\n",
        "            a[k]=L[i]\n",
        "            i+=1\n",
        "            k+=1\n",
        "        while(j<n2):\n",
        "            a[k]=R[j]\n",
        "            j+=1\n",
        "            k+=1\n",
        "    def merge_sort(a,l,r):\n",
        "        if l==r:\n",
        "            return\n",
        "        mid=(l+r)//2\n",
        "        merge_sort(a,l,mid)\n",
        "        merge_sort(a,mid+1,r)\n",
        "        merge(a,l,mid,r)\n",
        "\n",
        "    merge_sort(li,0,len(li)-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLwm6p-2ftMg"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords  \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.stem import PorterStemmer\n",
        "import matplotlib.pyplot as plt\n",
        "movies = pd.read_csv(\"movies.csv\")\n",
        "ratings = pd.read_csv(\"ratings.csv\")\n",
        "dataset = movies.merge(ratings)\n",
        "dataset = dataset.loc[:,[\"userId\",\"movieId\",\"title\",\"genres\",\"rating\"]]\n",
        "df_ratings = dataset.loc[:,[\"title\",\"rating\"]].groupby(\"title\").mean()\n",
        "genres = dataset[\"genres\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "senR20p-fwrg"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "li = []\n",
        "for i in range(len(genres)):\n",
        "    temp = genres[i].split(\"|\")\n",
        "    for j in range(len(temp)):\n",
        "        temp[j] = lemmatizer.lemmatize(temp[j])\n",
        "    li.append(\" \".join(temp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLcQkrfRfzy1"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer()\n",
        "X = cv.fit_transform(li).toarray()\n",
        "\n",
        "genres = pd.DataFrame(X,columns=cv.get_feature_names())\n",
        "dataset = dataset.iloc[:,:-2]\n",
        "new_dataset = dataset.join(genres)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kr2PtjNhmjN"
      },
      "source": [
        "new_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d43b19R5h-VY"
      },
      "source": [
        "\n",
        "users = new_dataset.drop([\"movieId\",\"title\"],axis=1)\n",
        "users_moviemat = users.groupby(\"userId\").sum()\n",
        "X = users_moviemat.iloc[:,:].values\n",
        "users_moviemat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbEanuPmiBKd"
      },
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "classifier = NearestNeighbors()\n",
        "classifier.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n1mg3guiFf9"
      },
      "source": [
        "uid = int(input(\"Enter User Id \"))\n",
        "li = classifier.kneighbors([X[uid-1]],n_neighbors=5,return_distance=False)\n",
        "current_user = new_dataset.loc[new_dataset[\"userId\"]==li[0][0],:][\"title\"].values\n",
        "similar_user = new_dataset.loc[new_dataset[\"userId\"]==li[0][1],:][\"title\"].values\n",
        "movies_list = [movie for movie in similar_user if movie not in current_user]\n",
        "sort_movies_by_year(movies_list)\n",
        "for i in range(len(movies_list)):\n",
        "    movies_list[i] = (movies_list[i], df_ratings['rating'][df_ratings.index == movies_list[i]].values[0])\n",
        "print(\"Recommended Movies are: \")\n",
        "movies_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdWZaptRiLm1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords  \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.stem import PorterStemmer\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "\n",
        "movies = pd.read_csv(\"movies.csv\")\n",
        "ratings= pd.read_csv(\"ratings.csv\")\n",
        "description= pd.read_csv(\"description.csv\")\n",
        "\n",
        "\n",
        "description_j = description[[\"imdb_title_id\",\"original_title\",\"description\",\"language\"]]\n",
        "imdb = pd.DataFrame(description_j)\n",
        "\n",
        "\n",
        "imdb_eng = imdb[imdb[\"language\"] == \"English\" ]\n",
        "imdb_eng.tail()\n",
        "imdb_eng.dropna(inplace= True)\n",
        "print(imdb_eng.isnull().values.any())\n",
        "print(len(imdb_eng))\n",
        "imdb_2 = imdb_eng.iloc[15000:]\n",
        "\n",
        "test = imdb_2.reset_index(drop = True)\n",
        "test = test.drop(\"imdb_title_id\",axis = 1)\n",
        "test.set_index(\"original_title\",drop = True,inplace = True)\n",
        "\n",
        "\n",
        "stop_w = set(stopwords.words('english'))\n",
        "stop_words = []\n",
        "for word in stop_w:\n",
        "  stop_words.append(word)\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "ps = PorterStemmer() \n",
        "i=0\n",
        "stop_copy=set(stop_words)\n",
        "\n",
        "i = 0\n",
        "for text in test[\"description\"]:\n",
        "  text=str(text)\n",
        "  text=set(text.split(\" \"))\n",
        "  text=text-stop_copy\n",
        "  a = list(text)\n",
        "  w = []\n",
        "  for word in a:\n",
        "    word_s = ps.stem(word)\n",
        "    word_l = lemmatizer.lemmatize(word_s)\n",
        "    w.append(word_l)\n",
        "  sentence = \"\"\n",
        "  for word in w:\n",
        "    sentence = sentence + word\n",
        "    sentence = sentence + \" \"\n",
        "  test[\"description\"][i]= sentence\n",
        "  i=i+1\n",
        "\n",
        "#test.head()\n",
        "#production_company\n",
        "#director\n",
        "#writer\n",
        "#actors\n",
        "#reviews_from_users\n",
        "#reviews_from_critics\n",
        "\n",
        "test.dropna(inplace= True)\n",
        "test.isnull().values.any()\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfv = TfidfVectorizer(min_df = 3 , max_features = None , strip_accents = \"unicode\", analyzer = \"word\" , ngram_range = (1,3))\n",
        "test2 = test\n",
        "test2[\"description\"] = test2[\"description\"].fillna(\" \")\n",
        "tfv_matrix = tfv.fit_transform(test2[\"description\"])\n",
        "tfv_matrix.shape\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "sigmoid_sim = cosine_similarity(tfv_matrix, tfv_matrix)\n",
        "\n",
        "dic1 = {}\n",
        "dic2 = {}\n",
        "indices = pd.DataFrame(test.index)\n",
        "for i,text in enumerate(indices[\"original_title\"]):\n",
        "  dic1[text] = i\n",
        "  dic2[i] = text\n",
        "print(dic1[\"True Blue\"])\n",
        "print(dic2[1])\n",
        "indices.head()\n",
        "#sigmoid_sim[0]\n",
        "\n",
        "def recommend_system(title,sigmoid_sim):\n",
        "  index = dic1[title]\n",
        "  sig_scores = list(enumerate(sigmoid_sim[index]))\n",
        "  scores = sorted(sig_scores,key= lambda x : x[1],reverse=True)\n",
        "  scores = scores[1:11]\n",
        "  movies_indics = [i[0] for i in scores]\n",
        "  movies = [dic2[i] for i in movies_indics]\n",
        "  return movies\n",
        "\n",
        "a = recommend_system(\"The Samaritan\",sigmoid_sim)\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "xzPFmdJMHg3X",
        "outputId": "2ee759eb-24ba-49d6-a25a-9d078a421b3a"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords  \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "\n",
        "imdb_indian = imdb[imdb[\"country\"] == \"India\"]\n",
        "print(imdb_indian.isnull().values.any())\n",
        "imdb_indian.dropna(inplace= True)\n",
        "print(imdb_indian.isnull().values.any())\n",
        "print(len(imdb_indian))\n",
        "print(\"\\n\")\n",
        "\n",
        "#imdb_indian.tail(100)\n",
        "imdb_telugu = imdb_indian[imdb_indian[\"language\"] == \"Telugu\"]\n",
        "#imdb_telugu.tail(100)\n",
        "\n",
        "test = imdb_indian.reset_index(drop = True)\n",
        "test = test.drop(\"imdb_title_id\",axis = 1)\n",
        "test.set_index(\"original_title\",drop = True,inplace = True)\n",
        "test[\"genre\"] = test[\"genre\"].str.split(\",\")\n",
        "test[\"writer\"] = test[\"writer\"].str.split(',')\n",
        "test[\"actors\"] = test[\"actors\"].str.split(',')\n",
        "test[\"director\"] = test[\"director\"].str.split(',')\n",
        "test[\"production_company\"] = test[\"production_company\"].str.split(',')\n",
        "\n",
        "stop_w = set(stopwords.words('english'))\n",
        "stop_words = []\n",
        "for word in stop_w:\n",
        "  stop_words.append(word)\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "ps = PorterStemmer() \n",
        "i=0\n",
        "stop_copy=set(stop_words)\n",
        "\n",
        "i = 0\n",
        "for text in test[\"description\"]:\n",
        "  text=str(text)\n",
        "  text=set(text.split(\" \"))\n",
        "  text=text-stop_copy\n",
        "  a = list(text)\n",
        "  w = []\n",
        "  for word in a:\n",
        "    word_s = ps.stem(word)\n",
        "    word_l = lemmatizer.lemmatize(word_s)\n",
        "    w.append(word_l)\n",
        "  sentence = \"\"\n",
        "  for word in w:\n",
        "    sentence = sentence + word\n",
        "    sentence = sentence + \" \"\n",
        "  test[\"description\"][i]= sentence\n",
        "  i=i+1\n",
        "\n",
        "test.dropna(inplace= True)\n",
        "test.isnull().values.any()\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfv = TfidfVectorizer(min_df = 3 , max_features = None , strip_accents = \"unicode\", analyzer = \"word\" , ngram_range = (1,3))\n",
        "test2 = test\n",
        "test2[\"description\"] = test2[\"description\"].fillna(\" \")\n",
        "tfv_matrix = tfv.fit_transform(test2[\"description\"])\n",
        "tfv_matrix.shape\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "sigmoid_sim = cosine_similarity(tfv_matrix, tfv_matrix)\n",
        "\n",
        "dic1 = {}\n",
        "dic2 = {}\n",
        "indices = pd.DataFrame(test.index)\n",
        "for i,text in enumerate(indices[\"original_title\"]):\n",
        "  dic1[text] = i\n",
        "  dic2[i] = text\n",
        "print(dic1[\"Andaz\"])\n",
        "print(dic2[1])\n",
        "indices.head()\n",
        "#sigmoid_sim[0]\n",
        "\n",
        "def recommend_system(title,sigmoid_sim):\n",
        "  index = dic1[title]\n",
        "  sig_scores = list(enumerate(sigmoid_sim[index]))\n",
        "  scores = sorted(sig_scores,key= lambda x : x[1],reverse=True)\n",
        "  scores = scores[1:11]\n",
        "  movies_indics = [i[0] for i in scores]\n",
        "  movies = [dic2[i] for i in movies_indics]\n",
        "  return movies\n",
        "\n",
        "a = recommend_system(\"Gang Leader\",sigmoid_sim)\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-09af39492a1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mimdb_indian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"country\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"India\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimdb_indian\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mimdb_indian\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'imdb' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoJbQuZ0nf19"
      },
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "li = []\n",
        "dataset = movies.merge(ratings)\n",
        "dataset = dataset.loc[:,[\"userId\",\"movieId\",\"title\",\"genres\",\"rating\"]]\n",
        "df_ratings = dataset.loc[:,[\"title\",\"rating\"]].groupby(\"title\").mean()\n",
        "genres = dataset[\"genres\"]\n",
        "for i in range(len(genres)):\n",
        "    temp = genres[i].split(\"|\")\n",
        "    for j in range(len(temp)):\n",
        "        temp[j] = lemmatizer.lemmatize(temp[j])\n",
        "    li.append(\" \".join(temp))\n",
        "cv = CountVectorizer()\n",
        "X = cv.fit_transform(li).toarray()\n",
        "\n",
        "genres = pd.DataFrame(X,columns=cv.get_feature_names())\n",
        "dataset = dataset.iloc[:,:-2]\n",
        "new_dataset = dataset.join(genres)\n",
        "users = new_dataset.drop([\"movieId\",\"title\"],axis=1)\n",
        "users_moviemat = users.groupby(\"userId\").sum()\n",
        "X = users_moviemat.iloc[:,:].values\n",
        "users_moviemat\n",
        "classifier = NearestNeighbors()\n",
        "classifier.fit(X)\n",
        "uid = int(input(\"Enter User Id \"))\n",
        "li = classifier.kneighbors([X[uid-1]],n_neighbors=5,return_distance=False)\n",
        "current_user = new_dataset.loc[new_dataset[\"userId\"]==li[0][0],:][\"title\"].values\n",
        "similar_user = new_dataset.loc[new_dataset[\"userId\"]==li[0][1],:][\"title\"].values\n",
        "movies_list = [movie for movie in similar_user if movie not in current_user]\n",
        "sort_movies_by_year(movies_list)\n",
        "for i in range(len(movies_list)):\n",
        "    movies_list[i] = (movies_list[i], df_ratings['rating'][df_ratings.index == movies_list[i]].values[0])\n",
        "print(\"Recommended Movies are: \")\n",
        "movies_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD3KIY24rbUf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}